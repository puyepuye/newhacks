{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### details: \n",
    "##### - load in data\n",
    "##### - write our own classification neural network\n",
    "##### - testing OpenCV\n",
    "##### - importing pretrained model (VGG16)\n",
    "##### - Incorporating dataset with VGG16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv(\"clothing-dataset-master/images.csv\")\n",
    "info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(path):\n",
    "\n",
    "    image = Image.open(path)\n",
    "    bw_image = image.convert('L')\n",
    "    bw_image = bw_image.resize((96, 96))\n",
    "    pixel_values = np.array(bw_image).flatten()\n",
    "    pixel_list = pixel_values.tolist()\n",
    "\n",
    "    return pixel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs_path = glob.glob(r'/Users/clairejaroonjetjumnong/Documents/Projects/NewHacks/clothing-dataset-master/images/*')\n",
    "img_names = [img.split('/')[-1] for img in all_imgs_path]\n",
    "img_names = [img.split('.')[0] for img in img_names]\n",
    "\n",
    "big_list = [convert_data(path) for path in all_imgs_path]\n",
    "df = pd.DataFrame(big_list)\n",
    "df['image'] = img_names\n",
    "for img in img_names:\n",
    "    matching_rows = info[info['image'] == img]\n",
    "    if not matching_rows.empty:\n",
    "        label = matching_rows['label'].values[0]\n",
    "        df.loc[df['image'] == img, 'label'] = label\n",
    "    else:\n",
    "        # Handle the case where there's no matching entry in 'info' for the image\n",
    "        print(f\"No label found for image: {img}\")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='any')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df['label'].unique().tolist()\n",
    "print(unique_values)\n",
    "print(df['label'].value_counts())\n",
    "labels_to_exclude = ['Not sure', 'Longsleeve','Hat','Skirt', 'Polo', 'Undershirt', 'Blazer', 'Hoodie', 'Body', 'Other', 'Top', 'Blouse', 'Skip']\n",
    "df = df[~df['label'].isin(labels_to_exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                transforms.Resize((96,96)), \n",
    "                transforms.ToTensor(), \n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #Normalize the image #RGB 3 channels withnin [-1,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to the 'label' column\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Fit the label encoder to the class labels and transform them to numerical values\n",
    "encoded_labels = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Get the mapping between class labels and their corresponding numerical values\n",
    "label_mapping = {encoded_label: label for label, encoded_label in zip(df['label'], encoded_labels)}\n",
    "print(label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"96clothes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs_path = [f'/Users/clairejaroonjetjumnong/Documents/Projects/NewHacks/clothing-dataset-master/images/{img}.jpg' for img in df['image']]\n",
    "all_labels = df['label_encoded'].to_list()\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "class DataSetClass(data.Dataset):\n",
    "    def __init__(self, img_paths, labels, transform):\n",
    "        self.imgs = img_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transform\n",
    "    def __getitem__(self, index):               \n",
    "        img = self.imgs[index]                  #Slice the data according to the index, and then return the data -> to tensor\n",
    "        label = self.labels[index]\n",
    "        pil_img = Image.open(img).convert('RGB')             \n",
    "        data = self.transforms(pil_img)\n",
    "        return data, label\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "food_dataset = DataSetClass(all_imgs_path, all_labels, transform)\n",
    "food_datalodaer = data.DataLoader(\n",
    "                            food_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True\n",
    ")\n",
    "# Visualize the data\n",
    "imgs_batch, labels_batch = next(iter(food_datalodaer))\n",
    "print(imgs_batch.shape)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, (img, label) in enumerate(zip(imgs_batch[:6], labels_batch[:6])):\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Class: {all_labels[label]}\")  # Display the label as the title\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.permutation(len(all_imgs_path))\n",
    "\n",
    "all_imgs_path = np.array(all_imgs_path)[index]\n",
    "all_labels = np.array(all_labels)[index]\n",
    "print(\"Total Number of Samples: \",len(all_imgs_path))\n",
    "\n",
    "#80% as train\n",
    "s = int(len(all_imgs_path)*0.7)\n",
    "print(\"Numbers of Pic for the Training Set: \",s)\n",
    "\n",
    "train_imgs = all_imgs_path[:s]\n",
    "train_labels = all_labels[:s]\n",
    "test_imgs = all_imgs_path[s:]\n",
    "test_labels = all_labels[s:]\n",
    "\n",
    "trainset = DataSetClass(train_imgs, train_labels, transform) #TrainSet TensorData\n",
    "testset = DataSetClass(test_imgs, test_labels, transform) #TestSet TensorData\n",
    "trainloader = data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True) #TrainSet Labels\n",
    "testloader = data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True) #TestSet Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df['label'].unique()\n",
    "print(len(unique_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) #input: 3, output: 6, kernel_size: 5, current img size: 92\n",
    "        self.pool = nn.MaxPool2d(2, 2) #kernel_size: 2, stride: 2, , current img size: 46\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) #input: 6, output: 16, kernel_size: 5, current img size: 42\n",
    "        self.fc1 = nn.Linear(16 * 21 * 21, 120) #input: 16*21*21, output: 120\n",
    "        self.fc2 = nn.Linear(120, 84) #input: 120, output: 84\n",
    "        self.fc3 = nn.Linear(84, len(label_mapping)) #input:84, output:2 -> 2 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply the convolutional layer followed by relu activation and max-pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # flatten the output tensor before passing it to the fully connected layers\n",
    "        x = x.view(-1, 16 * 21 * 21)\n",
    "        # apply the fully connected later followed by relu activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # apply the last fully connected layer with no activation function\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# create an instance of the net class and send it to the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = Net().to(device)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# use cross entropy loss because easy to compare probability distribution, closer to the correct label means lower loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Adam (Adaptive learning and Momentum)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# loop over the dataset for 20 epochs\n",
    "for epoch in range(15):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        if device.type == 'cuda':\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # apply the model\n",
    "        outputs = net(inputs)\n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        # update the model's parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # printing epoch, batch, running lsoss\n",
    "        if i % 20 == 19: \n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20:.3f}')\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it on the entire validation set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        if device.type == 'cuda':\n",
    "            images = images.to(device)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the validation images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in unique_values}\n",
    "total_pred = {classname: 0 for classname in unique_values}\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            preds.append(prediction)\n",
    "            trues.append(label)\n",
    "            if label == prediction:\n",
    "                correct_pred[unique_values[label]] += 1\n",
    "            total_pred[unique_values[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname} is {accuracy} %')\n",
    "\n",
    "preds = [i.item() for i in preds]\n",
    "trues = [i.item() for i in trues]\n",
    "\n",
    "# T-Shirt       1011\n",
    "# Pants          692\n",
    "# Shoes          431\n",
    "# Shirt          378\n",
    "# Dress          357\n",
    "# Outwear        312\n",
    "# Shorts         308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show result\n",
    "import random\n",
    "imgs_batch, labels_batch = next(iter(testloader))\n",
    "index0 = random.randint(0,31)\n",
    "img,label = imgs_batch[0], labels_batch[0]\n",
    "img = img.to(device)\n",
    "outputs = net(img)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print('True class:', label_mapping[int(label)], 'predicted:', label_mapping[int(predicted)])\n",
    "\n",
    "img = img.cpu()\n",
    "img = img.permute(1, 2, 0).numpy()\n",
    "# plt.subplot(2, 3)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "path = 'archive/hoodietest2.jpeg'\n",
    "\n",
    "# TestDataset class that resembles the Dataset class above but no labels\n",
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_paths[index]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "test_imgs_path = glob.glob(path)\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize((96,96)), \n",
    "                transforms.ToTensor(), \n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "test_dataset = TestDataset(test_imgs_path, transform=transform)\n",
    "testloader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "results = []\n",
    "# evaluation\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "        inputs = batch\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        results.extend(predicted.cpu().numpy())\n",
    "        \n",
    "\n",
    "print(label_mapping[int(results[0])])\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Open and display the image\n",
    "image = Image.open(path)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Turn off axis labels and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_path = 'fashionlol.pth'\n",
    "torch.save(net.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "cv2.namedWindow(\"test\")\n",
    "\n",
    "img_counter = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imshow(\"test\", frame)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # SPACE pressed\n",
    "        img_counter += 1\n",
    "\n",
    "cam.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "plt.imshow(frame_rgb)\n",
    "plt.axis('off')  # Turn off axis labels and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the image data\n",
    "height, width, channels = frame_rgb.shape\n",
    "\n",
    "# Calculate the coordinates of the center pixel\n",
    "center_x = width // 2\n",
    "center_y = height // 2\n",
    "\n",
    "# Get the pixel values at the center\n",
    "center_pixel = frame_rgb[center_y, center_x]\n",
    "\n",
    "# 'center_pixel' contains the RGB values of the center pixel\n",
    "print('Center Pixel RGB:', center_pixel)\n",
    "\n",
    "color_image = [[center_pixel]]\n",
    "# Display the color using Matplotlib\n",
    "plt.imshow(color_image)\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_screen_shot(frame):\n",
    "    '''return classification from '''\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize((96,96)), \n",
    "                    transforms.ToTensor(), \n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    pil_image = Image.fromarray((frame * 255).astype('uint8'))\n",
    "    transformed_tensor = transform(pil_image)\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load('fashionlol.pth'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            inputs = batch\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            results.extend(predicted.cpu().numpy())\n",
    "            \n",
    "    label_mapping = {encoded_label: label for label, encoded_label in zip(df['label'], encoded_labels)}\n",
    "    return label_mapping[int(results[0])]\n",
    "\n",
    "def get_class_from_screen_shot(frame):\n",
    "    '''return classification from '''\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize((96,96)), \n",
    "                    transforms.ToTensor(), \n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    pil_image = Image.fromarray((frame * 255).astype('uint8'))\n",
    "    transformed_tensor = transform(pil_image)\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load('fashionlol.pth'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            inputs = batch\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            results.extend(predicted.cpu().numpy())\n",
    "            \n",
    "    label_mapping = {encoded_label: label for label, encoded_label in zip(df['label'], encoded_labels)}\n",
    "    return label_mapping[int(results[0])]\n",
    "\n",
    "get_class_from_screen_shot(frame_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def path_classi(path):\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 6, 5) #input: 3, output: 6, kernel_size: 5, current img size: 92\n",
    "            self.pool = nn.MaxPool2d(2, 2) #kernel_size: 2, stride: 2, , current img size: 46\n",
    "            self.conv2 = nn.Conv2d(6, 16, 5) #input: 6, output: 16, kernel_size: 5, current img size: 42\n",
    "            self.fc1 = nn.Linear(16 * 21 * 21, 120) #input: 16*21*21, output: 120\n",
    "            self.fc2 = nn.Linear(120, 84) #input: 120, output: 84\n",
    "            self.fc3 = nn.Linear(84, 7) #input:84, output:7 -> 7 classes\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = x.view(-1, 16 * 21 * 21)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    class TestDataset(data.Dataset):\n",
    "        def __init__(self, file_paths, transform=None):\n",
    "            self.file_paths = file_paths\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.file_paths)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            img_path = self.file_paths[index]\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    test_imgs_path = glob.glob(path)\n",
    "    BATCH_SIZE = 10\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize((96,96)), \n",
    "                    transforms.ToTensor(), \n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    test_dataset = TestDataset(test_imgs_path, transform=transform)\n",
    "    testloader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    results = []\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load('fashionlol.pth'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            inputs = batch\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            results.extend(predicted.cpu().numpy())\n",
    "            \n",
    "    label_mapping = {1: 'Longsleeve', 7: 'T-Shirt', 6: 'Shorts', 5: 'Shoes', 4: 'Shirt', 0: 'Dress', 3: 'Pants', 2: 'Outwear'}\n",
    "    return label_mapping[int(results[0])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from PIL import Image\n",
    "\n",
    "def path_classi(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((224, 224))\n",
    "    x = np.array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    model = VGG16(weights='imagenet')\n",
    "\n",
    "    predictions = model.predict(x)\n",
    "\n",
    "    decoded_predictions = decode_predictions(predictions, top=5)[0]\n",
    "    \n",
    "    return decoded_predictions[0][1]\n",
    "\n",
    "path = \"archive/myclothes.jpg\"\n",
    "image = Image.open(path)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "path_classi(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils import data\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "# import glob\n",
    "# import torch\n",
    "# from torch.utils import data\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# from torchvision import transforms\n",
    "# import torchvision\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def path_classi(path):\n",
    "\n",
    "#     class Net(nn.Module):\n",
    "#         def __init__(self):\n",
    "#             super(Net, self).__init__()\n",
    "#             self.conv1 = nn.Conv2d(3, 6, 5) #input: 3, output: 6, kernel_size: 5, current img size: 92\n",
    "#             self.pool = nn.MaxPool2d(2, 2) #kernel_size: 2, stride: 2, , current img size: 46\n",
    "#             self.conv2 = nn.Conv2d(6, 16, 5) #input: 6, output: 16, kernel_size: 5, current img size: 42\n",
    "#             self.fc1 = nn.Linear(16 * 21 * 21, 120) #input: 16*21*21, output: 120\n",
    "#             self.fc2 = nn.Linear(120, 84) #input: 120, output: 84\n",
    "#             self.fc3 = nn.Linear(84, 7) #input:84, output:2 -> 2 classes\n",
    "\n",
    "#         def forward(self, x):\n",
    "#             # apply the convolutional layer followed by relu activation and max-pooling\n",
    "#             x = self.pool(F.relu(self.conv1(x)))\n",
    "#             x = self.pool(F.relu(self.conv2(x)))\n",
    "#             # flatten the output tensor before passing it to the fully connected layers\n",
    "#             x = x.view(-1, 16 * 21 * 21)\n",
    "#             # apply the fully connected later followed by relu activation\n",
    "#             x = F.relu(self.fc1(x))\n",
    "#             x = F.relu(self.fc2(x))\n",
    "#             # apply the last fully connected layer with no activation function\n",
    "#             x = self.fc3(x)\n",
    "#             return x\n",
    "\n",
    "#     class TestDataset(data.Dataset):\n",
    "#         def __init__(self, file_paths, transform=None):\n",
    "#             self.file_paths = file_paths\n",
    "#             self.transform = transform\n",
    "\n",
    "#         def __len__(self):\n",
    "#             return len(self.file_paths)\n",
    "\n",
    "#         def __getitem__(self, index):\n",
    "#             img_path = self.file_paths[index]\n",
    "#             img = Image.open(img_path).convert('RGB')\n",
    "#             if self.transform:\n",
    "#                 img = self.transform(img)\n",
    "#             return img\n",
    "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     test_imgs_path = glob.glob(path)\n",
    "#     BATCH_SIZE = 10\n",
    "\n",
    "#     transform = transforms.Compose([\n",
    "#                     transforms.Resize((96,96)), \n",
    "#                     transforms.ToTensor(), \n",
    "#                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#     ])\n",
    "\n",
    "#     test_dataset = TestDataset(test_imgs_path, transform=transform)\n",
    "#     testloader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "#     results = []\n",
    "#     model = Net()\n",
    "#     model.load_state_dict(torch.load('fashionlol.pth'))\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch in testloader:\n",
    "#             inputs = batch\n",
    "#             inputs = inputs.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             results.extend(predicted.cpu().numpy())\n",
    "            \n",
    "#     label_mapping = {1: 'Longsleeve', 7: 'T-Shirt', 6: 'Shorts', 5: 'Shoes', 4: 'Shirt', 0: 'Dress', 3: 'Pants', 2: 'Outwear'}\n",
    "#     return label_mapping[int(results[0])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 61/109 [===============>..............] - ETA: 21:13 - loss: 1.8755 - accuracy: 0.2785"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import glob\n",
    "\n",
    "data = pd.read_csv(\"96clothes.csv\")\n",
    "\n",
    "all_imgs_path = [f'/Users/clairejaroonjetjumnong/Documents/Projects/NewHacks/clothing-dataset-master/images/{img}.jpg' for img in data['image']]\n",
    "all_labels = data['label_encoded'].to_list()\n",
    "\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "batch_size = 32\n",
    "num_classes = 7\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(image_width, image_height))\n",
    "    img = img_to_array(img)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "labels = to_categorical(all_labels, num_classes=num_classes)\n",
    "\n",
    "def data_generator(image_paths, labels, batch_size):\n",
    "    num_samples = len(image_paths)\n",
    "    while True:\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_paths = image_paths[i : i + batch_size]\n",
    "            batch_labels = labels[i : i + batch_size]\n",
    "            \n",
    "            batch_images = [preprocess_image(image_path) for image_path in batch_paths]\n",
    "            yield np.array(batch_images), batch_labels\n",
    "\n",
    "train_generator = data_generator(all_imgs_path, labels, batch_size)\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(image_width, image_height, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "predictions = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the custom model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on your custom dataset\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(all_imgs_path) // batch_size,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('custom_clothing_vgg16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
